# 我的面经 · 算法

本人面经之 **算法** 分栏，后续补充。



# 投递经验（无反应的没写上去）

1. 百度 NLP 补录（2020，暴毙）
2. 博彦科技（投的时候不知道是外包，问了句就没理我了）
9. 软通（外包，要现场面，拒绝）
10. 杭州华为OD（凉）
5. 每日互动（在面）



# 机器学习（Machine Learning）

## 资料

- E:\资料\计算机\软件\ML
- E:\资料\计算机\软件\NLP\极客时间NLP训练营\机器学习面试100题\机器学习测试题.pdf
- 网站
  - AI Studio
  - ApacheCN
  - 极客时间（笔记：https://shimo.im/folder/gPjYtx9DJQW63gpQ）
  - 深度之眼
  - [王的机器](https://mp.weixin.qq.com/mp/homepage?__biz=MzIzMjY0MjE1MA==&hid=6&sn=d350f682d7c1d07153ff26ad3d572219&scene=1&devicetype=android-28&version=27000c35&lang=en&nettype=WIFI&ascene=7&session_us=gh_86283b3f44d0&wx_header=1 )
- 参考书（纸书、电子书）
  - 百面机器学习
  - 机器学习 - 周志华
  - 统计学习方法2 - 李航

## **知识点**

招聘常见要求

- HMM
- CRF
- SVM
- LR
- DT
- BOOST
- K-MEANS
- GBT
- TensorFlow / Pytorch / Keras

## **问题**

### 预测评估

- 精确率/查准率（Precision）、准确率（Accuracy）和召回率/查全率（Recall）？https://www.zhihu.com/question/19645541
  - Precision：你觉得的阳性里头，对了多少（真阳/猜阳）
  - Accuracy：有多少判断正确（猜对/全体）
  - Recall：正样本猜对多少（真阳/正样本）
  - 推荐理解方法：
    - Google 机器学习线图
    - 知乎 饼图
    - 原创：脑内模拟 Excel 筛法
      - 想象一堆数据，最右边两列分别是——label、predict
      - Precision 即筛出 predict 的 1，看其中 1-1 的占比
      - Accuracy 即看整表，看其中 1-1 和 0-0 的占比
      - Recall 即筛出 label 的 1，看其中 1-1 的占比
- 机器学习性能评估指标？ROC曲线，PR曲线，AUC等。http://charleshm.github.io/2016/03/Model-Performance/
- 宏平均：先求每个分类准确率，再求平均 https://blog.csdn.net/weixin_42864175/article/details/90521758
- 微平均：直接求准确率

### Pipeline

Pipeline（摘自深度之眼NLP工具库）

- ![文本分类pipeline](E:\资料\计算机\软件\NLP\深度之眼NLP工具库\play-nlp\images\文本分类pipeline.png)



每个步骤概览：

1. **数据预处理**：
    - 你的数据可能比较脏，比如带html标签、不合法的数据，你需要去除；
    - 你的文本数据可能需要分词，然后再去掉一些停用词（`的` `了`等）及标点；
2. **特征构造**：
    - 一般情况我们会使用`词特征`作为文本分类的特征
    - 你还可以增加其它人为定义的特征，比如：文本长度、n-gram特征（将连续的n个词当成一个词特征）等
3. **特征选择**：
    - 用词做特征，不做特征选择，很容易出现上万维、甚至几十万维， 这对计算来说可能是个灾难。即使计算资源充足，那也是对资源的浪费，因为真正对分类起作用的词，可能就只是一少部分；
    - 经常使用卡方检验、互信息等指标筛选1000~5000维的特征；
4. **权重计算**：
    - 使用了词特征，还有一个重要的点就是如何为每个特征赋值，常见的权重有：TF（词频）、TFIDF（词频 * 倒排文档频率）
    - 至此，我们得到了**样本特征矩阵**；
5. **归一化**：
    - 在实践中，我们往往需要对连续的特征进行标准化和归一化。即让不同特征的取值范围差别不能过大；
    - 标准化和归一化可以：加快模型训练收敛速度、可能会提高模型精度；
    - 对于离散的特征，我们需要使用OneHot进行编码；
6. **数据集划分**：
    - 我们往往需要使用留出法或者交叉验证法，对数据进行训练集、验证集、测试集的划分
    - 训练集用来训练模型、验证集用来调参、测试集用来评估模型泛化效果；
7. **训练分类模型**：
    - 特征矩阵已经就绪，接下来，就是要选择一个分类模型，比如：SVM或LR或者集成模型RF，然后训练模型；
8. **模型评估**：
    - 如何衡量模型的好坏呢？我们自然想到精度（acc）：对是对，错是错，对的样本数 / 整个样本数 ；
    - 对于类别分布不怎么均匀的情况，精度并不怎么靠谱，理想的指标是：准确率（准不准）、召回率（全不全） 以及 F1（两者的折中）；
9. **参数搜索**：
    - 有了指标后，一个分类模型可能有几十个参数，我们该如何选择某个特定的参数组合，使得此时的模型效果最好呢？
    - 可以使用网格搜索，在限定的参数空间内进行调参；也可以使用随机搜索，每次随机选择特定参数组合，然后取n次里面最好的参数组合；
10. **保存模型**：
    - 选择了最优参数的模型，我们需要将其保存下来，以供后续加载使用；
    - 可以用Python的pickle库持久化模型对象；
11. **预测**：
    - 你可以加载上一步保存的模型，对新的数据进行离线label预测；
    - 你还可以将已加载的模型预测功能封装成HTTP服务，提供即时预测功能；

### One-hot

One-Hot编码将离散特征转换为机器学习算法易于利用的一种形式，因为：

- **很多机器学习模型假设所有特征的取值是连续的**，即它们之间具有可比较的大小关系。比如对于：特征词"开心"，在文档3出现4次、在文档1出现0次，是可以被拿来比较的；
- **离散特征并不具备大小可比的关系**，你强行将 `'知乎'` `'微博'` `'天涯'` 变成id形式的 `1` `2` `3` 输入模型，会误导模型；
- **离散特征经过One-hot后，便具备了连续特征的可比较关系**；

### Embedding

### Word2Vec

https://blog.csdn.net/qq_35268841/article/details/107065297

### 标准化和归一化

标准化和归一化让不同特征的取值差别不过大

- 加快模型训练收敛速度
  - 如不归一化，梯度迭代时可能会偏离最小值的方向，走很多弯路，即训练时间变长
  - 归一化后目标函数会变圆
- 可能会提高模型精度
  - 特征之间较大的尺度差异会误导模型，降低模型精度
- 标准化：$X_{norm}=\frac{X-X_{mean}}{X_{std}}$
- 最大最小值归一化（相对最大绝对值归一化更常用）：$X_{norm}=\frac{X-X_{min}}{X_{max}-X_{min}}$
- 最大绝对值归一化：$X_{norm}=\frac{X}{|X_{max}|}$
- 对比
  - 相比最大绝对值归一化，**最小最大值归一化**更常用些；
  - 使用归一化来缩放数据，则**数据将更集中在均值附近**，这是由于归一化的缩放是“拍扁”统一到区间（仅由极值决定）；
  - 标准化的缩放是更加“弹性”和“动态”的，因为它**保留了样本的特征值分布情况**；
  - 所以**归一化不能很好地处理离群值，而标准化对异常值的鲁棒性强**，在许多情况下，标准化优于归一化；

### 词袋模型和TFIDF

**词袋模型**忽略掉文本的语法和语序等要素，**将一个文本看作是若干个词汇的集合，文档中每个单词的出现都是独立的**。

- **可以使用ngram捕获局部词序**

用词袋法表示的权重矩阵，是通过词频来表示的。在实践中，我们的**权重矩阵往往是通过tfidf来计算的**，它在词频的基础上，还考虑了文档频率。**tfidf是一种非常普遍、有效的权重计算方式**。公式如下：
$$
TFIDF = TF \cdot IDF
$$

- **TF**：词频(term frequency)，它展示了**文档中每个词出现的频率，频率越高，就越重要**。$n$ 表示某个词在文档中出现的次数，$N$ 表示文档中所有词出现的次数总和，这是一个归一化的过程，目的是消除文档篇幅长短上的差异。公式如下：

$$
TF = \frac {n}{N}
$$



- **IDF**：逆文档频率(inverse document frequency)，它展示了**某个词在所有文档中出现的频率，文档频率越低，即逆文档频率越高，则词越重要**；$D$ 表示语料中所有的文档总数，$d$ 表示语料中出现某个词的文档数量，公式中的是 1 为了防止分母为 0 的情况（加不加 1 即是否平滑），$log$ 是以 $10$ 为底的对数；IDF值计算公式如下：

$$
IDF = log[\frac{D}{1+d}]
$$




总之，TFIDF的意思是说：**词的重要性，随着 在一个文档中出现的次数增多 而变大，随着 在所有文档中的出现次数增加 而变小**；

### C-BOW 和 Skip-gram

- C-BOW（拿上下文预测中心词，更快更好），exp变换，为了保证正数且单调
- Skip-gram（拿中心词预测上下文，生僻字有优势）
  - 1相关，0不相关（注意负采样，比如说菠萝、石油）
  - 问题：要处理大矩阵乘法，速度慢，消耗内存多
- 可以看 莫烦 PYTHON



### HHM

 隐马尔可夫模型（核心即 p(x, y) 和 三元组 (π, A, B)），三个用途：

- 样本生成，(π, A, B) -> x, y
- 模型训练，x, y -> (π, A, B) 
- 序列预测，(π, A, B) , x -> y

### sigmoid 和 softmax 的区别

https://baijiahao.baidu.com/s?id=1636737136973859154&wfr=spider&for=pc

- Sigmoid =多标签分类问题=多个正确答案=非独占输出（例如胸部X光检查、住院）
- Softmax =多类别分类问题=只有一个正确答案=互斥输出（例如手写数字，鸢尾花）

### ReLU 及其变种

除了ReLU（线性整流函数，Rectified Linear Unit，又称修正线性单元）外，还存在Leaky ReLU、PReLU、RReLU 和 BReLU 等激活函数

https://cloud.tencent.com/developer/article/1597479

### 优化器的选择

- SGD（适合CV）
- SGD + Momentum
- Adagrad
- Adam（适合NLP，或者用AdamW）

### 梯度爆炸和梯度消失

- 梯度爆炸：给一个上限
- 梯度消失：改变初始值，或改变步长

### 知识蒸馏

用大模型训练小模型，是迁移学习的特例

### K-means

肘部法则分层

### 聚类

最近两点不断聚类 设定最高值

### PCA

无监督降维，投影到特征向量（主成分）上，而特征值代表数据投影后的方差大小。（选较大的特征值的特征向量，投影）
$$
\Sigma=\begin{bmatrix}X轴上方差&X和Y协方差\\X和Y协方差&Y\end{bmatrix}
$$

### Dropout

分测试阶段和训练阶段，测试阶段要乘以保留的概率值，以保证输出与期望的基本一致。

https://www.zhihu.com/question/402485242

https://zhuanlan.zhihu.com/p/38200980

### 相似度

$$
\begin{align}
余弦距离&=\frac{向量 1和向量 2的内积}{向量1的长度\times向量2的长度}\\
\end{align}
$$

### 交叉熵、散度

**正规定义：**使用专门为另一个分布制作的密码表来发送某个分布中事件的信息，此时信息的平均编码长度定义为**交叉熵（cross-entropy）。**

两个数据分布分别为 p 分布和 q 分布的话，则 p 分布对 q 分布的交叉熵公式如下：
$$
H_p(q)=\sum\limits_{x}q(x)\times\log_2(\frac{1}{p(x)})
$$
而 q 分布对 p 分布的交叉熵公式如下：
$$
H_q(p)=\sum\limits_{x}p(x)\times\log_2(\frac{1}{q(x)})
$$
**Kullback-Leibler 散度（KL 散度）**是熵与交叉熵之间的差值。称之为散度而不是距离是因为距离是对称的，而散度可以是不对称的。

p 分布对 q 分布的 KL 散度定义为：
$$
KL_p(q)=H_p(q)-H(q)=\sum\limits_{x}q(x)\times\log_2(\frac{q(x)}{p(x)})
$$
而 q 分布对 p 分布的 KL 散度定义为：
$$
KL_q(p)=H_p(p)-H(p)=\sum\limits_{x}p(x)\times\log_2(\frac{p(x)}{q(x)})
$$
上面的 log~2~ xxx 其实是描述两种分布编码的长度差异。分布 p 和 q 差别越大，那么之间的 KL 散度（两个都越大）也就越大。

**交叉熵**，即使用针对另一分布制作的密码表对某个分布内的事件进行通讯时的长度，其组成分为两部分:

1. 使用针对本分布密码表进行通讯时所需的最短平均编码长度，即**熵**
2. 因使用针对其他分布的密码表而导致的多出的部分，即 **KL 散度**

数学表达式如下：
$$
交叉熵_p(q) =熵(q)+散度_p(q)\\ 
交叉熵_q(p) =熵(p) +散度_q(p)
$$
![image-20210308220408231](C:\Users\11046\AppData\Roaming\Typora\typora-user-images\image-20210308220408231.png)

### Trade-off（取舍）

https://mp.weixin.qq.com/s/OZ2xuLQSpgS5T6RiRqw8uw

### 集成学习

Bagging：弱依赖，并行，如随机森林

Boosting：强依赖，串行，如 AdaBoost、GBDT

集成学习应**好而不同**，Bagging更关注不同，Boosting更关注好。





### 开课吧算法面试题

- E:\资料\计算机\软件\算法\面试题.pdf

- [ ] 全概率公式和贝叶斯公式。
- [ ] 模型训练为何引入偏差（bias）和方差（variance）？
- [ ] 介绍一下 CRF/朴素贝叶斯/EM/最大熵模型/马尔可夫随机场/混合高斯模型 。
- [ ] 如何解决过拟合问题？
- [ ] One-hot 的作用是什么？为什么不直接使用数字作为表示？
- [ ] 决策树和随机森林的区别是什么？
- [x] 朴素贝叶斯为什么朴素（Naive）？
- [ ] kmeans 初始点除了随机选取之外还有什么方法？
- [ ] LR 明明是分类模型为什么叫回归？
- [ ] 梯度下降如何并行化？
- [ ] LR 中的 L1/L2 正则项是什么？
- [ ] 简述决策树构建的过程。
- [ ] 解释 Gini 系数。
- [ ] 决策树的优缺点。
- [ ] 出现估计概率值为 0 怎么处理？
- [ ] 随机森林的生成过程。
- [ ] 介绍一下 Boosting 的思想。
- [ ] GBDT 中的 Tree 是什么 Tree？有什么特征？
- [ ] XGBoost 对比 GBDT/Boosting Tree 有了哪些方面的优化？
- [ ] 什么叫最优化超平面？
- [ ] 什么是支持向量？
- [ ] SVM 如何解决多分类问题？
- [ ] 核函数的作用是什么？

### 面试题库

- E:\资料\计算机\软件\AI\学习大礼包\面试题库.pdf

### 牛客算法工程师面试题库

- E:\资料\计算机\软件\AI\AI_ibooker群文件\面试刷题\机器学习算法工程师面试题库.pdf



# 特征工程（Feature Engineering）

## 资料

- 参考书（纸书、电子书）
- E:\资料\计算机\软件\NLP\极客时间NLP训练营\机器学习面试100题\特征工程测试题.pdf

## **知识点**

- Pandas
- PCA
- LDA

## **问题**

- 数据清洗（插值法等）

- 数据增强

## 开课吧算法面试 pdf 题目

- [ ] 怎么去除 DataFrame 里的缺失值？
- [ ] 特征无量纲化的常见操作方法？
- [ ] 如何对类别变量进行独热编码？
- [ ] 如何把”年龄“字段按照我们的阈值分段？
- [ ] 如何根据变量相关性画出热力图？
- [ ] 如何把分布修正为类正态分布？
- [ ] 怎么简单使用 PCA 来划分数据且可视化呢？
- [ ] 怎么简单使用 LDA 来划分数据且可视化呢？



# 深度学习（Deep Learning）

## 资料

- E:\资料\计算机\软件\DL
- E:\资料\计算机\软件\NLP\极客时间NLP训练营\机器学习面试100题\深度学习测试题.pdf
- 网站
  - AI Studio
  - ApacheCN
  - 极客时间（笔记：https://shimo.im/folder/gPjYtx9DJQW63gpQ）
  - 深度之眼
- 参考书（纸书、电子书）
  - 百面深度学习
  - 百度 PaddlePaddle

## **知识点**

招聘常见要求

- CNN
- RNN、LSTM

## **问题**

### 网络

[RNN神经网络](https://mp.weixin.qq.com/s?__biz=MzIzMjY0MjE1MA==&mid=2247488290&idx=1&sn=427aee1a0afaf4c4f34b6d451de8bd3d&scene=19#wechat_redirect)中 O ——> O 中，箭头意味着使用了变化（即乘了个矩阵）



## 开课吧算法面试 pdf 题目

- [ ] 你觉得 batch-normalization 过程是怎样的？
- [ ] 激活函数有什么用？常见的激活函数的区别是什么？
- [ ] Softmax 的原理是什么？有什么作用？
- [ ] CNN 的平移不变性是什么？如何实现的？
- [ ] AlexNet、VGG、GoogleNet、ResNet 等网络之间的区别是？
- [ ] 残差网络为什么能解决梯度消失的问题？
- [ ] LSTM 为什么能解决梯度消失/爆炸的问题？
- [ ] Attention 对比 RNN 和 CNN，分别有哪些点你觉得的优势？
- [ ] 写出 Attention 的公式。
- [ ] Attention 机制中的 q、k、v 分别代表什么？
- [ ] 为什么 self-attention 可以代替 seq2seq？



# 自然语言处理（Natural Language Processing）

## 资料

- E:\资料\计算机\软件\NLP
- 网站
  - AI Studio
  - ApacheCN
  - 极客时间（笔记：https://shimo.im/folder/gPjYtx9DJQW63gpQ）
  - 深度之眼
- 参考书（纸书、电子书）
- 花书

## **知识点**

招聘常见要求

- 预处理
  - 分词
  - 关键词提取
  - 停止词
  
- 词向量
  - One-hot
  - Word2vec
  - 词袋
  - embedding
  - fasttext
  - glove
  - ELMO
  
- TF-IDF

- LSTM

- Transformer

- Attention

- Bert

- 知识图谱 https://mp.weixin.qq.com/s/9Zg3YMbFAfVAQFh7m1Tn7A

- 关系抽取

  - https://blog.csdn.net/qq_35268841/article/details/107063066

  - https://zhuanlan.zhihu.com/p/237452918

- 文本分类

- 语义分析

- 对话系统

- 序列问题
  - 分词
  - 词性标注
  - 命名实体识别
  - 依存句法分析

## **问题**

- pointwise（以样本为单位，分类型）pairwise（判断相关性，比较型）
- LSTM：https://www.jianshu.com/p/4b4701beba92

## 开课吧算法面试 pdf 题目

- [ ] 解释 GolVe 的损失函数。

- [ ] 为什么 GolVe 会用的相对比 Word2vec 少？

- [ ] 层次 Softmax 流程？

- [ ] 负采样流程？

- [ ] 怎样衡量学到的 embedding 的好坏？

- [ ] 阐述 CRF 原理。

- [ ] 详述 LDA 原理。

- [ ] LDA 中的主题矩阵如何计算？

- [ ] 马尔科夫链的收敛性质？

- [ ] MCMC 中什么叫做马尔科夫链采样过程？

- [ ] 给定平稳矩阵如何得到概率分布样本集？

- [ ] 什么叫做坐标转换采样？

- [ ] 变分推断 EM 算法？

- [ ] Bert 的双向体现在什么地方？

- [ ] Bert 是怎样预训练的？

- [ ] 手写一个 multi-head attention。

  

# 推荐系统（Recommendation System）

## 资料

- E:\资料\计算机\软件\DL

## **知识点**

- 

## **问题**

## 开课吧算法面试 pdf 题目

- [ ] DNN 与 DeepFM 之间的区别？
- [ ] 你在使用 DeepFM 时是如何处理欠拟合和过拟合问题的？
- [ ] DeepFM 的 embedding 初始化有什么值得注意的地方吗？
- [ ] YoutubeNet 变长数据如何处理的？
- [ ] YoutubeNet 如何避免百万量级的 softmax 问题的？
- [ ] 推荐系统有哪些常见的评测指标？
- [ ] MLR 的原理是什么？做了哪些优化？



# 计算机视觉（Computer Vision）

## 资料

- E:\资料\计算机\软件\DL

## **知识点**

- 

## **问题**

## 开课吧算法面试 pdf 题目

- [ ] 常见的模型加速方法？
- [ ] 目标检测中如何有效解决常见的前景少背景多的问题？
- [ ] 目标检测里有什么情况是 SSD、YoLov3、Faster R-CNN 等不能解决的？假设网络拟合能力无限强。
- [ ] ROIPool 和 ROIAlign 的区别？
- [ ] 介绍常见的梯度下降优化方法。
- [ ] Detection 你觉得还有哪些可做的点？
- [ ] mini-Batch SGD 相对于 GD 有什么优点？
- [ ] 人体姿态估计的两个主流做法是啥？简单介绍下。
- [ ] 卷积的实现原理以及如何快速高效实现局部 weight sharing 的卷积操作方式？
- [ ] Cycle CAN 的生成效果为何一般都是位置不变，纹理变化，为什么不能产生不同位置的生成效果？



# 数学

## 资料

- E:\资料\数学
- 普林斯顿三剑客
- 程序员的数学三件套
- 统计：https://www.zhihu.com/people/michael-song-20/zvideos

## **知识点**

- 

## **问题**