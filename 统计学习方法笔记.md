# 统计学习方法笔记

这是本人对统计学习方法学习中摘录的一些知识点

本机文件路径：E:\资料\计算机\软件\AI\深度之眼\李航《统计学习方法》带学训练营



# 极大似然估计（MLE）

- **MLE**，**M**aximum **L**ikelihood **E**stimation
- 未知参数$\theta$是定值
- 目标：位置参数$\theta$使得数据集D发生的概率最大，$max\ P(D|\theta)$

## 下溢出（Underflow）

数据过小，$lg$ 一下（取对数）

## 梯度下降法（**G**radient **D**escent）

## 过拟合（**Over Fitting**）

- Train 过度 —> Early Stop
- 模型太复杂 —> 正则化等，降低复杂度、NN层数减少、剪枝等

## 欠拟合（**Under Fitting**）

## 超平面（Hyperplane）

- 函数间距
- 集合间距



# 感知机（**Perceptron**）

- **思想：分错的点和直线距离求和最小**

- 维度d大 —> 对偶

- 样本N大 —> 原始

- 为了避免0处不可微，$sign(wx+b)$脱去了$sign$

  <table>
      <tr>
          <th align="center" colspan="4">class Perceptron</th> 
      </tr>
      <tr>
          <th align="left" colspan="4">Parameter（参数）</th>  
      </tr>
      <tr>   
          <td>序号</td> 
          <td>名称</td> 
          <td>默认值</td> 
          <td>可选值</td> 
      </tr>
      <tr>   
          <td>1</td> 
          <td>penalty（正则化项）</td> 
          <td>None</td> 
          <td>'l2' or 'l1' or 'elasticnet'</td> 
      </tr>
      <tr>   
          <td>2</td> 
          <td>alpha(正则化系数）</td> 
          <td>0.0001</td> 
          <td></td> 
      </tr>
      <tr>   
          <td>3</td> 
          <td>eta0（学习率）</td> 
          <td>1</td> 
          <td>(0,1]</td> 
      </tr>
      <tr>   
          <td>4</td> 
          <td>max_iter（迭代次数）</td> 
          <td>5</td> 
          <td>若tol不为None则为1000</td> 
      </tr>
      <tr>   
          <td>5</td> 
          <td>tol（终止条件）</td> 
          <td>None</td> 
          <td>(previous_loss-loss)&lt;lot</td> 
    </tr>
  </table>



## 正则化（**Regularization**）

- L1：特征值更稀疏，可为零
- L2：权值更均匀，接近零

- 系数过小，约束无用

- 系数过大，约束过度欠拟合


## 对偶形式（**Dual Form**）



# K近邻（KNN）

- **KNN，K N**earest **N**eighbors

- **思想：物以类聚**

- K指最近的K个样本

- K值过小，太敏感，噪声容易影响预测出错

- K值过大，不相似的也会囊括进去，使预测错误

- 实际中先取个小的K值，交叉验证再调大

- K近邻没有显式的训练过程

- 分类决策规则：多数表决

  <table>
      <tr>
          <th align="center" colspan="4">KNeighborsClassifier</th> 
      </tr>
      <tr>
          <th align="left" colspan="4">一、Parameter（参数）</th>  
      </tr>
      <tr>   
          <td>序号</td> 
          <td>名称</td> 
          <td>默认值</td> 
          <td>可选值</td> 
      </tr>
      <tr>   
          <td>1</td> 
          <td>n_neighbors（近邻数）</td> 
          <td>5</td> 
          <td>正整数[1,N]</td> 
      </tr>
      <tr>   
          <td>2</td> 
          <td>weights（近邻权重）</td> 
          <td>uniform-权重一样</td> 
          <td>distance-越近权重越大<br>[callable]-自定义</td> 
      </tr>
      <tr>   
          <td>3</td> 
          <td>algorithm（算法）</td> 
          <td>auto-自动选择</td> 
          <td>brute-暴力求解<br>kd_tree-KD树<br>ball_tree-球树<br>注：当数据量比较小的时候，不管设定哪种算法最终都是暴力求解</td> 
      </tr>
      <tr>   
          <td>4</td> 
          <td>leaf_size</td> 
          <td>30</td> 
          <td>叶子节点数量的阈值</td> 
      </tr>
      <tr>   
          <td>5</td> 
          <td>p</td> 
          <td>2</td> 
          <td rowspan="2">距离度量：欧氏距离</td> 
      </tr>
      <tr>   
          <td>6</td> 
          <td>metric</td> 
          <td>mincowski</td> 
      </tr>
      <tr>
      	<td>7</td>
      	<td>n_jobs</td>
          <td>None</td> 
          <td>并行搜索<br>None：1 个进程<br>-1：所有进程</td> 
      </tr>
  <table>
  	<tr>
          <th align="left" colspan="3">二、Methods（方法-函数）</th>  
      </tr>
      <tr>   
          <td>序号</td> 
          <td>名称</td> 
          <td>作用</td> 
      </tr>
      <tr>   
          <td>1</td> 
          <td>fit</td> 
          <td>确定适用算法</td> 
      </tr>
      <tr>   
          <td>2</td> 
          <td>predict</td> 
          <td>对测试点进行分类</td> 
      </tr>
      <tr>   
          <td>3</td> 
          <td>predict_proba</td> 
          <td>对测试点属于不同分类的概率</td> 
      </tr>
      <tr>   
          <td>4</td> 
          <td>score</td> 
          <td>输入测试集，评价训练效果</td> 
      </tr>
      <tr>   
          <td>5</td> 
          <td>kneighbors</td> 
          <td rowspan="2">返回k近邻点</td> 
      </tr>
      <tr>   
          <td>6</td> 
        <td>kneighbors_graph</td> 
  </table>



## 范数（**Norm**）

- 欧氏距离$L_1$范数
- 曼哈顿/城市街区距离$L_2$范数
- 切比雪夫/棋盘距离一致范数$L_\infty$

$$
L_p(x_i, x_j) = (\sum\limits_{l=1}^n\left|x_i^{(l)}-x_j^{(l)}\right|^p)^\frac{1}{p}
$$

## Kd树（**Kd Tree**）

- 步骤：①构造；②搜索
- K指K维空间

- 特征空间（Feature Space）划分：根据$x^{(l)}\text{，}l=(j\text{ mod } k)+1$ 中位点轴垂线，划分空间

- 减少计算距离的次数
- 但维数接近训练实例数时，效率速降，逼近线性搜索（20维以内，N>>2d；高维用Kd树）
- Kd树采用了特殊的结构存储训练数据
- 最近邻搜索

## 指示函数（Indicator Function）

指示函数  $I_A(x)=\begin{cases}1&if\ x\in A\\0&if\ x\not\in A\end{cases}$（实际中基本不用，因为只知道错了，不知道错哪了）



# 朴素贝叶斯（**Naive Bayes**）

- $P(A|B)=\frac{P(AB)}{P(B)}=\frac{P(B|A)P(A)}{P(B)}$

- MNIST（虽然手写数字像素相关，但**强制假设独立**——即所谓“朴素”，否则算不出来）


## 贝叶斯估计（Bayesian Estimation）

- 分子加$\lambda$
- 分母加$S_j\lambda，其中S_j为x_i可取特征数$
- 拉普拉斯平滑（Laplace Smoothing），防止特征过少，分母为零
- 虽加入值，但不改变主要概率分布
- 位置参数$\theta$本身服从一定概率分布
- 目标：数据集D发生的情况下，哪一个$\theta$发生的概率最大，$max\ P(\theta|D)$

## 概率（Probability）

- 先验概率（Prior Probability）：以往经验得到的概率，因—>果

- 后验概率（Posterior Probability）：得到的结果去计算，最有可能的概率，果—>因

- 条件概率（Conditional Probability）：在某条件下事件发生的概率。

- 后验概率在某些情况下是条件概率的一种

- 后验概率最大化含义：期望风险最小化

  <table>
      <tr>
          <th align="center" colspan="3">相关 Sklearn 实现</th> 
      </tr>
      <tr>   
          <td>GaussianNB</td> 
          <td>高斯分布</td> 
          <td>连续变量，如人的身高</td> 
      </tr>
      <tr>   
          <td>MultinomialNB</td> 
          <td>多项分布</td> 
          <td>离散变量，如男女</td> 
      </tr>
      <tr>   
          <td>ComplementNB</td> 
          <td>改进的多项分布</td> 
          <td>用于不平衡数据集</td> 
      </tr>
      <tr>   
          <td>BernoulliNB</td> 
          <td>伯努利分布</td> 
          <td>X<sub>i</sub> 只能取 (0,1)</td> 
      </tr>
  </table>
  <table>
        <tr>
            <th align="center" colspan="3">MultinomialNB</th> 
        </tr>
    	  <tr>
            <th align="left" colspan="3">Methods（方法-函数）</th>  
        </tr>
        <tr>   
            <td>序号</td> 
            <td>名称</td> 
            <td>作用</td> 
        </tr>
        <tr>   
            <td>1</td> 
            <td>fit</td> 
            <td>确定适用算法</td> 
        </tr>
        <tr>   
            <td>2</td> 
            <td>predict</td> 
            <td>对测试点进行分类</td> 
        </tr>
        <tr>   
            <td>3</td> 
            <td>predict_proba</td> 
            <td>对测试点属于不同分类的概率</td> 
        </tr>
        <tr>   
            <td>4</td> 
            <td>score</td> 
            <td>输入测试集，评价训练效果</td> 
        </tr>
        <tr>   
            <td>5</td> 
            <td>kneighbors</td> 
            <td>数据分块学习</td> 
        </tr>
  </table>



# 决策树（Decision Tree）

- **DT，Decision Tree**

- 思想：以树为结构基础，对特征判断进入分支，直到叶节点

- 左右子树较平衡，则认为决策树构建较好

- ID3 算法（**I**terative **D**ichotomiser 3，信息增益算法）

- ID4.5 算法（**I**terative **D**ichotomiser 4.5，信息增益比算法）

## 剪枝（Tree Pruning）

- 降低模型复杂度，有预剪枝、后剪枝之分

- Sklearn.tree.DecisionTreeClassifier 采用的是 CART 算法，预剪枝、后剪枝

- CART（Classification and Regression Trees，分类与回归树）

  <table>
      <tr>
      	<th align="center" colspan="5">Sklearn.tree.DecisionTreeClassifier</th>
      </tr>
        	  <tr>
            <th align="left" colspan="3">Parameters（参数）</th>  
        </tr>
        <tr>   
            <td>序号</td> 
            <td>名称</td> 
            <td>作用</td> 
            <td>默认值</td> 
            <td>可选值</td> 
        </tr>
        <tr>   
            <td>1</td> 
            <td>criterion</td> 
            <td>度量分类质量</td> 
            <td>"gini"</td>
            <td>"entropy"</td>
        </tr>
        <tr>   
            <td>2</td> 
            <td>splitter</td> 
            <td>选择分类点</td> 
            <td>"best"</td>
            <td>"random"</td>
        </tr>
        <tr>   
            <td>3</td> 
            <td>max_depth</td> 
            <td>树最大深度</td> 
            <td>None</td>
            <td>int</td>
        </tr>
        <tr>   
            <td>4</td> 
            <td>min_samples_split</td> 
            <td>最小样本阈值</td> 
            <td>2</td>
            <td>int or float</td>
        </tr>
        <tr>   
            <td>5</td> 
            <td>min_samples_leaf</td> 
            <td>叶子节点最小样本</td> 
            <td>1</td>
            <td>int or float</td>
        </tr>
        <tr>   
            <td>6</td> 
            <td>min_weight_fraction_leaf</td> 
            <td>叶子节点最小样本权重</td> 
            <td>0</td>
            <td>float</td>
        </tr>
        <tr>   
            <td>7</td> 
            <td>max_features</td> 
            <td>考虑最大特征值</td> 
            <td>None</td>
            <td>int float "auto" "sqrt" "log2"</td>
        </tr>
        <tr>   
            <td>8</td> 
            <td>random_state</td> 
            <td>随机数种子</td> 
            <td>None</td>
            <td></td>
        </tr>
        <tr>   
            <td>9</td> 
            <td>max_leaf_nodes</td> 
            <td>最大叶子节点数量</td> 
            <td>None</td>
            <td>int</td>
        </tr>
        <tr>   
            <td>10</td> 
            <td>min_inpurity_decrease</td> 
            <td>split 损失阈值</td> 
            <td>0</td>
            <td>float</td>
        </tr>
        <tr>   
            <td>11</td> 
            <td>min_inpurity_split</td> 
            <td colspan="3">被 min_inpurity_decrease 替代，将被弃用</td> 
        </tr>
        <tr>   
            <td>12</td> 
            <td>class_weight</td> 
            <td>类别权重</td> 
            <td>None</td>
            <td>解决样本不均衡</td>
        </tr>
        <tr>   
            <td>13</td> 
            <td>presort</td> 
            <td>数据是否排序</td> 
            <td>FALSE</td>
            <td>bool</td>
        </tr>



## 信息熵（**Information Entropy**）

- 即信息的不确定性，熵为零时不确定性坍塌为零

- $H(X)=-\sum\limits_{i=1}^{n}p_i\log_2 p_i，其中\ 0\leqslant H(P)\leqslant \log n$

- 均匀分布时，熵最大 

- 关联交叉熵、KL散度


## 信息增益（**Information Gain**）

- 表示得知特征 X 的信息而使得类 Y 的信息的不确定性减少的程度
  $$
  g(D,A)=H(D)-H(D|A)
  $$

- 信息增益会导致节点倾向于划分特征取值数目较多的，因为会打散，即子树过多（例如，身份证、房子面积具体值），由此引入信息增益比（**Information Gain Ratio**）
  $$
  g_R(D,A)=\frac{g(D,A)}{H(D)}
  $$

- 为什么不用信息增益，网络还有言：==大数问题导致的概率是否准确问题==



# 逻辑斯谛回归（LR）

- **LR，Logistic Regression**

- 速度快、性能不错

- 逻辑斯谛回归定义 $P(Y=1|x)=\frac{\exp(w·x)}{1+\exp(w·x)}\ ; \ 
  P(Y=0|x)=\frac{1}{1+\exp(w·x)}$。图像：  

  ​                                         ![image-20210306201739787](C:\Users\11046\AppData\Roaming\Typora\typora-user-images\image-20210306201739787.png)





## 最大熵

- 

- 定义可以看出，连续可微，且具有概念意义

  

  
